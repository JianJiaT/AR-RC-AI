{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f8a8c87-49b1-4a2f-a149-da71d917cecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import sys\n",
    "import os\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "import copy\n",
    "from functools import partial\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01e0060f-e907-4cee-91a3-cbbcb2f201be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjjtj\\CG4002\\glove_data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\tjjtj\\\\CG4002\\\\glove_data\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29a0ac30-26b8-414e-8c3e-49279e28053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sum_features(input):\n",
    "    temp = input\n",
    "    for i in range(6):\n",
    "        curr_col = i * 4\n",
    "        mean = torch.mean(temp[:, curr_col]).item()\n",
    "        min = torch.min(temp[:, curr_col]).item()\n",
    "        max = torch.max(temp[:, curr_col]).item()\n",
    "        mean_col = torch.full((50, 1), mean)\n",
    "        min_col = torch.full((50, 1), min)\n",
    "        max_col = torch.full((50, 1), max)\n",
    "\n",
    "        left = temp[:, :curr_col+1]\n",
    "        right = temp[:, curr_col+1:]\n",
    "        temp = torch.cat((left, mean_col, min_col, max_col, right), dim=1)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4f37954-6544-4c06-82ae-cedc5627ed5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, labels_file, data_dir, means=None, std_devs=None):\n",
    "        if (data_dir == \"\"):\n",
    "            path = \"C:\\\\Users\\\\tjjtj\\\\CG4002\\\\glove_data\"\n",
    "        else:\n",
    "            path = \"C:\\\\Users\\\\tjjtj\\\\CG4002\\\\glove_data\\\\\" + data_dir\n",
    "        path_to_labels_file = path + \"\\\\\" + labels_file\n",
    "        self.data_labels = pd.read_csv(path_to_labels_file, header=None)\n",
    "\n",
    "        datas = []\n",
    "        self.datapoints = []\n",
    "        for i in range(len(self.data_labels)):\n",
    "            file_path = os.path.join(path, self.data_labels.iloc[i, 0])\n",
    "            \n",
    "            with open(file_path, encoding=\"utf-8\") as f:\n",
    "                file_content = f.read()\n",
    "            line_array = []\n",
    "            lines = file_content.strip().split(\"\\n\")\n",
    "            for line in lines:\n",
    "                num_array = []\n",
    "                nums = line.strip().split(\",\")\n",
    "                for num in nums:\n",
    "                    num_array.append(float(num))\n",
    "                line_array.append(num_array)\n",
    "            temp = torch.FloatTensor(line_array)\n",
    "            data = add_sum_features(temp)\n",
    "\n",
    "            label = self.data_labels.iloc[i, 1]\n",
    "\n",
    "            datapoint = [data, label]\n",
    "            datas.append(data)\n",
    "            self.datapoints.append(datapoint)\n",
    "\n",
    "        datas = torch.stack(datas)\n",
    "        \n",
    "        if (means is None):\n",
    "            means = []\n",
    "            for i in range(24):\n",
    "                means.append(torch.mean(datas[:, :, i]).item())\n",
    "        if (std_devs is None):\n",
    "            std_devs = []\n",
    "            for i in range(24):\n",
    "                std_devs.append(torch.std(datas[:, :, i]).item())\n",
    "        self.means = means\n",
    "        self.std_devs = std_devs            \n",
    "        \n",
    "        for datapoint in self.datapoints:\n",
    "            data = datapoint[0]\n",
    "            for i in range(24):\n",
    "                curr_data = data[:, i]\n",
    "                curr_data = torch.sub(curr_data, means[i])\n",
    "                curr_data = torch.div(curr_data, std_devs[i])\n",
    "                datapoint[0][:, i] = curr_data\n",
    "        \n",
    "        self.path = path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.datapoints[idx][0]\n",
    "        label = self.datapoints[idx][1]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38f7dfb4-4f52-4032-a216-59ef6ab3a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(dataset, batch_size):\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    train_set, valid_set = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "    trainloader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "    validloader = DataLoader(valid_set, batch_size, shuffle=True)\n",
    "    return trainloader, validloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a707887a-0445-4878-b0d8-1fba6b391e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=14):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = nn.RNN(25, hidden_size, batch_first = True)\n",
    "        self.classifier = nn.Linear(hidden_size, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, x = self.rnn(x)\n",
    "        x = self.classifier(x)\n",
    "        output = x\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00284c72-5052-41f5-9233-12f1ad9249ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (rnn): RNN(25, 100, batch_first=True)\n",
       "  (classifier): Linear(in_features=100, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load saved model\n",
    "net = Net(100)\n",
    "net.load_state_dict(torch.load(\"95%_train_acc_log_12_model_1_100_hidden.pt\", weights_only=True))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee183722-79b9-4af6-af09-64dd35f727c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = list(net.named_parameters())\n",
    "with open(\"model_description.txt\", \"w\") as f:\n",
    "    for param in params:\n",
    "        param_name = param[0]\n",
    "        f.write(param_name)\n",
    "        f.write(\" \")\n",
    "        f.write(str(param[1].size()))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(str(param[1].tolist()))\n",
    "        f.write(\"\\n\")\n",
    "        if (\"weight\" in param_name):\n",
    "            f.write(param_name + \"_transposed\")\n",
    "            f.write(\" \")\n",
    "            f.write(str(param[1].T.size()))\n",
    "            f.write(\"\\n\")\n",
    "            f.write(str(param[1].T.tolist()))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e0cea1b-9c63-41c1-ac74-2c4c579534b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"model_description.txt\", \"r\") as f:\n",
    "    contents = f.read()\n",
    "\n",
    "contents = contents.replace(\"[\", \"{\")\n",
    "contents = contents.replace(\"]\", \"}\")\n",
    "\n",
    "with open(\"model_description_cpp.txt\", \"w\") as f:\n",
    "    f.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1936a4a6-87cc-4c31-8aa7-89150150b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_description_cpp.txt\", \"r\") as f:\n",
    "    contents = f.read()\n",
    "    contents_list = contents.strip().split(\"\\n\")\n",
    "    RNN_INPUT_WEIGHTS_TRANSPOSED = contents_list[3]\n",
    "    RNN_HIDDEN_WEIGHTS_TRANSPOSED = contents_list[7]\n",
    "    RNN_INPUT_BIASES = contents_list[9]\n",
    "    RNN_HIDDEN_BIASES = contents_list[11]\n",
    "    CLASSIFIER_WEIGHTS_TRANSPOSED = contents_list[15]\n",
    "    CLASSIFIER_BIASES = contents_list[17]\n",
    "\n",
    "with open(\"log_12.txt\", \"r\") as f:\n",
    "    contents = f.read()\n",
    "    contents_list = contents.strip().split(\"\\n\")\n",
    "    for row in contents_list:\n",
    "        row_list = row.split()\n",
    "        if row_list[0] == \"Means\":\n",
    "            MEANS = row[row.find(\"[\"):]\n",
    "            MEANS = MEANS.replace(\"[\", \"{\")\n",
    "            MEANS = MEANS.replace(\"]\", \"}\")\n",
    "        elif row_list[0] == \"Std\" and row_list[1] == \"devs\":\n",
    "            STD_DEVS = row[row.find(\"[\"):]\n",
    "            STD_DEVS = STD_DEVS.replace(\"[\", \"{\")\n",
    "            STD_DEVS = STD_DEVS.replace(\"]\", \"}\")\n",
    "            break\n",
    "\n",
    "SEQ_LEN = 50\n",
    "NUM_INPUT_FEATURES = 7\n",
    "NUM_FEATURES = 25\n",
    "HIDDEN_SIZE = 100\n",
    "NUM_CLASSES = 8\n",
    "\n",
    "with open(\"C:\\\\Users\\\\tjjtj\\\\Desktop\\\\NUS\\\\Y4S1\\\\CG4002\\\\Project\\\\NN_Vitis\\\\hls_component\\\\consts.h\", \"w\") as f:\n",
    "    f.write(f\"const int SEQ_LEN = {SEQ_LEN};\\n\")\n",
    "    f.write(f\"const int NUM_INPUT_FEATURES = {NUM_INPUT_FEATURES};\\n\")\n",
    "    f.write(f\"const int NUM_FEATURES = {NUM_FEATURES};\\n\")\n",
    "    f.write(f\"const int HIDDEN_SIZE = {HIDDEN_SIZE};\\n\")\n",
    "    f.write(f\"const int NUM_CLASSES = {NUM_CLASSES};\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(f\"const float RNN_INPUT_WEIGHTS_TRANSPOSED[NUM_FEATURES][HIDDEN_SIZE] = {RNN_INPUT_WEIGHTS_TRANSPOSED};\\n\")\n",
    "    f.write(f\"float RNN_INPUT_BIASES[HIDDEN_SIZE] = {RNN_INPUT_BIASES};\\n\")\n",
    "    f.write(f\"const float RNN_HIDDEN_WEIGHTS_TRANSPOSED[HIDDEN_SIZE][HIDDEN_SIZE] = {RNN_HIDDEN_WEIGHTS_TRANSPOSED};\\n\")\n",
    "    f.write(f\"float RNN_HIDDEN_BIASES[HIDDEN_SIZE] = {RNN_HIDDEN_BIASES};\\n\")\n",
    "    f.write(f\"const float CLASSIFIER_WEIGHTS_TRANSPOSED[HIDDEN_SIZE][NUM_CLASSES] = {CLASSIFIER_WEIGHTS_TRANSPOSED};\\n\")\n",
    "    f.write(f\"float CLASSIFIER_BIASES[NUM_CLASSES] = {CLASSIFIER_BIASES};\\n\")\n",
    "    f.write(f\"float MEANS[NUM_FEATURES - 1] = {MEANS};\\n\")\n",
    "    f.write(f\"float STD_DEVS[NUM_FEATURES - 1] = {STD_DEVS};\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c10ff846-0e0b-4bee-b6d9-e18652c6c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7183306d-6e95-4d95-b5f6-eafcf891e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_test_input_file(gesture):\n",
    "    labels_file = gesture + \"_labels.csv\"\n",
    "    with open(labels_file, \"r\") as f:\n",
    "        content = f.read()\n",
    "        first_path = content.strip().split()[0][:-1]\n",
    "        label = content.strip().split()[1]\n",
    "\n",
    "    with open(first_path, \"r\") as f:\n",
    "        contents = f.read()\n",
    "        contents_list = contents.split()\n",
    "        contents_array = np.empty((50, 7))\n",
    "        index = 0 \n",
    "        for line in contents_list:\n",
    "            line_list = line.split(\",\")\n",
    "            contents_array[index] =  np.array(line_list)\n",
    "            index += 1    \n",
    "        shape = str(contents_array.shape)\n",
    "        \n",
    "        contents_array = np.array2string(contents_array, separator=\", \")\n",
    "        contents_array = contents_array.replace(\"\\n\", \"\")\n",
    "\n",
    "    filename = \"test_\" + gesture + \"_input\"\n",
    "    filename_with_extension = filename + \".txt\"\n",
    "    with open(filename_with_extension, \"w\") as f:\n",
    "        f.write(shape)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(label)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(contents_array)\n",
    "    \n",
    "    contents_array = contents_array.replace(\"[\", \"{\")\n",
    "    contents_array = contents_array.replace(\"]\", \"}\")\n",
    "\n",
    "    cpp_filename_with_extension = filename + \"_cpp\" + \".txt\"\n",
    "    with open(cpp_filename_with_extension, \"w\") as f:\n",
    "        f.write(shape)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(label)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(contents_array)\n",
    "        \n",
    "gestures = [\"stationary\", \"random\", \"wave\", \"punch\", \"swipe\", \"wipe\", \"shake\", \"hammer\"]\n",
    "for gesture in gestures:\n",
    "    write_test_input_file(gesture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9e44a96-65da-4cb6-8808-a529314ad390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_expanded_test_input_file(gesture):\n",
    "    labels_file = gesture + \"_labels.csv\"\n",
    "    with open(labels_file, \"r\") as f:\n",
    "        content = f.read()\n",
    "        first_path = content.strip().split()[0][:-1]\n",
    "        label = content.strip().split()[1]\n",
    "\n",
    "    with open(first_path, \"r\") as f:\n",
    "        contents = f.read()\n",
    "        contents_list = contents.split()\n",
    "        contents_array = np.empty((50, 7))\n",
    "        index = 0 \n",
    "        for line in contents_list:\n",
    "            line_list = line.split(\",\")\n",
    "            contents_array[index] =  np.array(line_list)\n",
    "            index += 1\n",
    "\n",
    "        in_means = np.mean(contents_array, axis=0)\n",
    "        in_mins = np.min(contents_array, axis=0)\n",
    "        in_maxes = np.max(contents_array, axis=0)\n",
    "        for i in range(6):\n",
    "            curr_col = i * 4\n",
    "            mean_column = np.full(50, in_means[i])\n",
    "            min_column = np.full(50, in_mins[i])\n",
    "            max_column = np.full(50, in_maxes[i])\n",
    "            contents_array = np.insert(contents_array, curr_col+1, mean_column, axis=1)\n",
    "            contents_array = np.insert(contents_array, curr_col+2, min_column, axis=1)\n",
    "            contents_array = np.insert(contents_array, curr_col+3, max_column, axis=1)\n",
    "\n",
    "        shape = str(contents_array.shape)\n",
    "        \n",
    "        contents_array = np.array2string(contents_array, separator=\", \", threshold=10000)\n",
    "        contents_array = contents_array.replace(\"\\n\", \"\")\n",
    "\n",
    "    filename = \"test_expanded_\" + gesture + \"_input\"\n",
    "    filename_with_extension = filename + \".txt\"\n",
    "    with open(filename_with_extension, \"w\") as f:\n",
    "        f.write(shape)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(label)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(contents_array)\n",
    "    \n",
    "    contents_array = contents_array.replace(\"[\", \"{\")\n",
    "    contents_array = contents_array.replace(\"]\", \"}\")\n",
    "\n",
    "    cpp_filename_with_extension = filename + \"_cpp\" + \".txt\"\n",
    "    with open(cpp_filename_with_extension, \"w\") as f:\n",
    "        f.write(shape)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(label)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(contents_array)\n",
    "        \n",
    "gestures = [\"stationary\", \"random\", \"wave\", \"punch\", \"swipe\", \"wipe\", \"shake\", \"hammer\"]\n",
    "for gesture in gestures:\n",
    "    write_expanded_test_input_file(gesture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7614284-6f06-4fae-a2b4-8e27caa0e908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transpose_matrix(matrix):\n",
    "    rows = len(matrix)\n",
    "    cols = len(matrix[0])\n",
    "    transposed_matrix = [[matrix[j][i] for j in range(rows)] for i in range(cols)]\n",
    "    return transposed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e65ba11-73e1-4775-b58b-ec5b85baa8c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def multiply(a, b):\n",
    "    def is_matrix(x):\n",
    "        return isinstance(x[0], list)\n",
    "\n",
    "    def to_column_matrix(vec):\n",
    "        return [[x] for x in vec]\n",
    "\n",
    "    def to_row_matrix(vec):\n",
    "        return [vec]\n",
    "\n",
    "    def dot_product(v1, v2):\n",
    "        return sum(x * y for x, y in zip(v1, v2))\n",
    "\n",
    "    def matrix_multiply(A, B):\n",
    "        rows_A, cols_A = len(A), len(A[0])\n",
    "        rows_B, cols_B = len(B), len(B[0])\n",
    "        \n",
    "        if cols_A != rows_B:\n",
    "            raise ValueError(\"Incompatible dimensions for multiplication\")\n",
    "\n",
    "        result = [[0 for _ in range(cols_B)] for _ in range(rows_A)]\n",
    "        for i in range(rows_A):\n",
    "            for j in range(cols_B):\n",
    "                for k in range(cols_A):\n",
    "                    result[i][j] += A[i][k] * B[k][j]\n",
    "        return result\n",
    "\n",
    "    a_is_matrix = is_matrix(a)\n",
    "    b_is_matrix = is_matrix(b)\n",
    "\n",
    "    if not a_is_matrix and not b_is_matrix:\n",
    "        if len(a) != len(b):\n",
    "            raise ValueError(\"Vectors must be the same length for dot product\")\n",
    "        return dot_product(a, b)\n",
    "\n",
    "    if not a_is_matrix:\n",
    "        a = to_row_matrix(a)\n",
    "    if not b_is_matrix:\n",
    "        b = to_column_matrix(b)\n",
    "\n",
    "    result = matrix_multiply(a, b)\n",
    "\n",
    "    if len(result) == 1 and len(result[0]) == 1:\n",
    "        return result[0][0]\n",
    "    elif len(result) == 1:\n",
    "        return result[0]\n",
    "    elif len(result[0]) == 1:\n",
    "        return [row[0] for row in result]\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b77ec06f-855a-4f15-9adc-62d9f4d35762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def elementwise_tanh(data):\n",
    "    if isinstance(data[0], list):\n",
    "        return [[math.tanh(x) for x in row] for row in data]\n",
    "    else:\n",
    "        return [math.tanh(x) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92b95854-ba23-42e2-b4ef-a6b5ad86d5a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_vectors(vector_A, vector_B):\n",
    "    result = [0 for _ in range(len(vector_A))]\n",
    "    for i in range(len(vector_A)):\n",
    "        result[i]= vector_A[i] + vector_B[i]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81955c4b-f397-485c-a3dc-468c3331348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sum_features(input):\n",
    "    temp = input\n",
    "    for i in range(6):\n",
    "        curr_col = i * 4\n",
    "        mean = torch.mean(temp[:, curr_col]).item()\n",
    "        min = torch.min(temp[:, curr_col]).item()\n",
    "        max = torch.max(temp[:, curr_col]).item()\n",
    "        mean_col = torch.full((50, 1), mean)\n",
    "        min_col = torch.full((50, 1), min)\n",
    "        max_col = torch.full((50, 1), max)\n",
    "\n",
    "        left = temp[:, :curr_col+1]\n",
    "        right = temp[:, curr_col+1:]\n",
    "        temp = torch.cat((left, mean_col, min_col, max_col, right), dim=1)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63108e25-4c57-4459-910c-94a884a5bb35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n",
      "Sim debug buffer (partial):[0.414622678611713, -0.06081006772415281, 0.4517822289211986, -0.058576991824177804, -0.2657550068098279, -0.025421263872602115, 0.39877024146280876, -0.010068004592120858, -0.4531850696439678, 0.2748962308075608]\n",
      "Sim output: [6.345718582265222, 3.1781436974756616, -1.187690121588394, -0.8102586179991862, -2.756175875238431, -1.413985281636051, -2.945513135454442, -1.5735499657458567]\n",
      "Sim prediction: 0\n",
      "Sim prediction runtime: 0.25113773345947266\n",
      "Actual output: tensor([[ 6.3457,  3.1781, -1.1877, -0.8103, -2.7562, -1.4140, -2.9455, -1.5735]])\n",
      "Actual prediction: tensor([0])\n",
      "Actual prediction runtime: 0.3084144592285156\n",
      "\n",
      "Label: 1\n",
      "Sim debug buffer (partial):[0.7090371814183664, -0.1332246480140507, 0.18923158418250563, 0.04594706438163598, -0.34590956248973825, 0.010461106241405074, 0.5117613852418765, 0.1936295102619912, -0.5396507231340169, 0.4252104852782542]\n",
      "Sim output: [5.583307187678921, 3.7533906945798603, -2.9733742071443325, -1.5730900478081964, -1.7462064804124198, -0.395230907992951, -2.2022890866501803, -1.470708635597879]\n",
      "Sim prediction: 0\n",
      "Sim prediction runtime: 0.27501940727233887\n",
      "Actual output: tensor([[ 5.5833,  3.7534, -2.9734, -1.5731, -1.7462, -0.3952, -2.2023, -1.4707]])\n",
      "Actual prediction: tensor([0])\n",
      "Actual prediction runtime: 0.09916925430297852\n",
      "\n",
      "Label: 2\n",
      "Sim debug buffer (partial):[-0.21734677849327472, 0.06289893217141064, -0.05144500387344614, -0.17632846812003358, 0.24252681566342502, 0.09018803717010661, 0.013421894761790969, -0.062453678482874336, 0.2984430156169677, 0.024997612113705703]\n",
      "Sim output: [-1.2970919648712238, -1.9677535992643858, 6.215275662817104, -0.7447498749439014, -0.7432224323633749, 0.10310362548932012, -0.2025127886802531, -0.8414913820014689]\n",
      "Sim prediction: 2\n",
      "Sim prediction runtime: 0.2521069049835205\n",
      "Actual output: tensor([[-1.2971, -1.9678,  6.2153, -0.7447, -0.7432,  0.1031, -0.2025, -0.8415]])\n",
      "Actual prediction: tensor([2])\n",
      "Actual prediction runtime: 0.10852599143981934\n",
      "\n",
      "Label: 3\n",
      "Sim debug buffer (partial):[-0.40117187232915436, 0.34575355161770627, 0.10278879919152903, 0.2997195582971759, 0.19451649168823643, 0.22705359340664874, -0.4416556550093492, 0.01863425497401867, 0.2459705745307561, -0.35088922933830713]\n",
      "Sim output: [-4.865426661014909, 0.18178978012544253, -0.7680184268389575, 6.046707208348677, -0.3377167346841161, -0.16438653032607026, -1.3725663650794249, 1.5569927888517952]\n",
      "Sim prediction: 3\n",
      "Sim prediction runtime: 0.24810361862182617\n",
      "Actual output: tensor([[-4.8654,  0.1818, -0.7680,  6.0467, -0.3377, -0.1644, -1.3726,  1.5570]])\n",
      "Actual prediction: tensor([3])\n",
      "Actual prediction runtime: 0.10600733757019043\n",
      "\n",
      "Label: 4\n",
      "Sim debug buffer (partial):[0.48109489585234944, 0.01891618912252927, -0.2108439229079048, -0.07947862551111193, 0.014718620681785364, -0.022726982046296217, 0.19435596940737596, 0.05665385269334178, -0.18380034147358162, 0.09112402149654268]\n",
      "Sim output: [0.004602108863329624, -2.449385435849802, -0.8698523384226335, -0.2883677444107522, 6.535877916761532, -0.668653037423697, -0.11842540432020016, -1.562094737817724]\n",
      "Sim prediction: 4\n",
      "Sim prediction runtime: 0.2538573741912842\n",
      "Actual output: tensor([[ 4.6022e-03, -2.4494e+00, -8.6985e-01, -2.8837e-01,  6.5359e+00,\n",
      "         -6.6865e-01, -1.1843e-01, -1.5621e+00]])\n",
      "Actual prediction: tensor([4])\n",
      "Actual prediction runtime: 0.10805296897888184\n",
      "\n",
      "Label: 5\n",
      "Sim debug buffer (partial):[-0.17992554355494203, 0.1679512045573366, 0.5460434047417411, -0.05469596909827449, -0.06227716483077164, 0.2062678135244304, 0.1285357536686605, 0.07302917226716361, 0.003092325077486538, 0.17671886517386737]\n",
      "Sim output: [-2.725491399990911, 1.1241412991348254, -0.006591488275970829, 0.8831971695603966, -1.5460734305704904, 5.4505369880502315, -2.1357921461800027, -0.978925193267961]\n",
      "Sim prediction: 5\n",
      "Sim prediction runtime: 0.2567298412322998\n",
      "Actual output: tensor([[-2.7255,  1.1241, -0.0066,  0.8832, -1.5461,  5.4505, -2.1358, -0.9789]])\n",
      "Actual prediction: tensor([5])\n",
      "Actual prediction runtime: 0.09644627571105957\n",
      "\n",
      "Label: 6\n",
      "Sim debug buffer (partial):[-1.7240934442405678, -1.0392086800099096, -0.7078641660093532, -0.3358333356612682, 0.432786707672834, -0.9936328213197243, -0.5682110545289023, -0.6423328266069611, 0.7095136996258445, -0.4436477690296552]\n",
      "Sim output: [-3.912891208989021, -0.4186897288300403, 1.6510374823115979, -1.329853413614051, -0.3425621157299145, -2.005709728947414, 6.3154338534327215, 0.6978277083613325]\n",
      "Sim prediction: 6\n",
      "Sim prediction runtime: 0.2424027919769287\n",
      "Actual output: tensor([[-3.9129, -0.4187,  1.6510, -1.3299, -0.3426, -2.0057,  6.3154,  0.6978]])\n",
      "Actual prediction: tensor([6])\n",
      "Actual prediction runtime: 0.10539555549621582\n",
      "\n",
      "Label: 7\n",
      "Sim debug buffer (partial):[0.24919851442163674, 0.03134863436396214, -0.25955393729075, 0.2692444752306815, -0.007573400276751646, 0.0008649227815478227, 0.01687152514754695, 0.045777410462089066, -0.021013520418426572, -0.14841805758501522]\n",
      "Sim output: [0.9887920974834836, -3.563677884605888, -0.294476255671579, 0.018443749957369762, 1.5469373718509418, -1.2447332500474322, -1.3195674167030256, 3.922140717513857]\n",
      "Sim prediction: 7\n",
      "Sim prediction runtime: 0.2388472557067871\n",
      "Actual output: tensor([[ 0.9888, -3.5637, -0.2945,  0.0184,  1.5469, -1.2447, -1.3196,  3.9221]])\n",
      "Actual prediction: tensor([7])\n",
      "Actual prediction runtime: 0.08966469764709473\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def forward_primitive(data_tensor, label, means, std_devs):\n",
    "    params_dict = dict(net.named_parameters())\n",
    "    \n",
    "    print(\"Label: \" + label)\n",
    "    test_input = data_tensor\n",
    "    inputs = test_input\n",
    "    time1 = time.time()\n",
    "    seq_len = inputs.size()[0] \n",
    "    \n",
    "    zeroes = []\n",
    "    zeroes.append([0.0 for j in range(100)]) # needs to match hidden_size of net\n",
    "    h_t_minus_1 = copy.deepcopy(zeroes)\n",
    "    h_t = copy.deepcopy(zeroes)\n",
    "\n",
    "    inputs = inputs.tolist()\n",
    "    for i in range(6):\n",
    "        curr_col = i * 4\n",
    "        col_values = []\n",
    "        for row in range(50):\n",
    "            col_values.append(inputs[row][curr_col])\n",
    "        mean = sum(col_values) / 50\n",
    "        minimum = min(col_values)\n",
    "        maximum = max(col_values)\n",
    "        for row in range(50):\n",
    "            inputs[row].insert(curr_col+1, mean)\n",
    "            inputs[row].insert(curr_col+2, minimum)\n",
    "            inputs[row].insert(curr_col+3, maximum)\n",
    "    \n",
    "    rnn_input_weights = params_dict[\"rnn.weight_ih_l0\"].tolist()\n",
    "    rnn_hidden_weights = params_dict[\"rnn.weight_hh_l0\"].tolist()\n",
    "    rnn_input_biases = params_dict[\"rnn.bias_ih_l0\"].tolist()\n",
    "    rnn_hidden_biases = params_dict[\"rnn.bias_hh_l0\"].tolist()\n",
    "    for t in range(seq_len):\n",
    "        input_t = inputs[t]\n",
    "        for i in range(24):\n",
    "            input_t[i] = input_t[i] - means[i]\n",
    "            input_t[i] = input_t[i] / std_devs[i]\n",
    "            \n",
    "        in_mult_weights = multiply(input_t, transpose_matrix(rnn_input_weights))\n",
    "        h_t_mult_weights = multiply(h_t_minus_1, transpose_matrix(rnn_hidden_weights))\n",
    "        temp1 = add_vectors(in_mult_weights, h_t_mult_weights)\n",
    "        temp2 = add_vectors(rnn_input_biases, rnn_hidden_biases)\n",
    "        h_t = add_vectors(temp1, temp2)\n",
    "        h_t = elementwise_tanh(h_t)\n",
    "        h_t_minus_1 = h_t\n",
    "        if t == 0:\n",
    "            print(\"Sim debug buffer (partial):\" + str(in_mult_weights[:10]))\n",
    "        # below are also part of the debug buffer, excluded here for visibility\n",
    "        #     print(h_t_mult_weights)\n",
    "        #     print(temp1)\n",
    "        #     print(rnn_input_biases)\n",
    "        #     print(rnn_hidden_biases)\n",
    "        #     print(temp2)\n",
    "        #     print(h_t)\n",
    "    \n",
    "    rnn_output = h_t\n",
    "    \n",
    "    rnn_classi_weights = params_dict[\"classifier.weight\"].tolist()\n",
    "    rnn_classi_bias = params_dict[\"classifier.bias\"].tolist()\n",
    "    rnn_out_mult_weights = multiply(rnn_output, transpose_matrix(rnn_classi_weights))\n",
    "    output = add_vectors(rnn_out_mult_weights, rnn_classi_bias)\n",
    "    print(\"Sim output: \" + str(output))\n",
    "    \n",
    "    max_conf = -9999\n",
    "    index = 0\n",
    "    max_conf_index = 0\n",
    "    for conf in output:\n",
    "        if (conf > max_conf):\n",
    "            max_conf_index = index\n",
    "            max_conf = conf\n",
    "        index += 1\n",
    "    prediction = max_conf_index\n",
    "    print(\"Sim prediction: \" + str(prediction))\n",
    "    print(\"Sim prediction runtime: \" + str(time.time() - time1))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        time1 = time.time()\n",
    "        expanded_input = add_sum_features(test_input)\n",
    "        for row in expanded_input:\n",
    "            for i in range(24):\n",
    "                curr_data = row[i]\n",
    "                curr_data = torch.sub(curr_data, means[i])\n",
    "                curr_data = torch.div(curr_data, std_devs[i])\n",
    "                row[i] = curr_data\n",
    "        actual_output = net(expanded_input)\n",
    "        _, predicted = torch.max(actual_output, 1)\n",
    "        \n",
    "        print(\"Actual output: \" + str(actual_output))\n",
    "        print(\"Actual prediction: \" + str(predicted))\n",
    "        print(\"Actual prediction runtime: \" + str(time.time() - time1) + \"\\n\")\n",
    "\n",
    "means = [0.46261587738990784, 0.46261587738990784, -0.6956228613853455, 1.6631245613098145, -0.1328771561384201, -0.1328771412372589, -1.018193006515503, 0.631430447101593, -0.09139232337474823, -0.09139233082532883, -0.9628987908363342, 0.866577684879303, -6.184083461761475, -6.184083461761475, -192.46563720703125, 171.81434631347656, -1.4080398082733154, -1.408039927482605, -212.31764221191406, 193.5176239013672, 0.7322556972503662, 0.732255756855011, -164.01646423339844, 193.4847412109375]\n",
    "std_devs = [1.0614013671875, 0.40623387694358826, 1.3376641273498535, 1.221408724784851, 0.8451341986656189, 0.5858472585678101, 1.2368916273117065, 0.8280805945396423, 1.1143925189971924, 0.874140202999115, 1.5301002264022827, 0.8200597763061523, 151.6263427734375, 25.81968116760254, 190.47702026367188, 165.2723846435547, 243.46754455566406, 29.674396514892578, 277.20880126953125, 262.9638366699219, 159.34500122070312, 30.97650909423828, 148.56300354003906, 194.8113555908203]\n",
    "gestures = [\"stationary\", \"random\", \"wave\", \"punch\", \"swipe\", \"wipe\", \"shake\", \"hammer\"]\n",
    "for gesture in gestures:\n",
    "    test_filename = \"test_\" + gesture + \"_input.txt\"\n",
    "        \n",
    "    with open(test_filename, \"r\") as f:\n",
    "        content = f.read()\n",
    "        content_list = content.split(\"\\n\")\n",
    "        shape = content_list[0]\n",
    "        label = content_list[1]\n",
    "        data = content_list[2]\n",
    "        data = data[1:-1]\n",
    "        data_array = np.empty((50, 7))\n",
    "        index = 0\n",
    "        for row_str in data.split('], ['):\n",
    "            row_str = row_str.strip('[] ')\n",
    "            row = [float(x) for x in row_str.split(', ')]\n",
    "            data_array[index] =  np.array(row)\n",
    "            index += 1\n",
    "        data_tensor = torch.from_numpy(data_array)\n",
    "        data_tensor = data_tensor.to(torch.float32)\n",
    "        forward_primitive(data_tensor, label, means, std_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6f04400-f852-41a8-8cf9-0af6e1b3001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2018 Stefano Nardo https://gist.github.com/stefanonardo\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e138052e-e286-4712-ba3e-4025f7c9a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sum_features(input):\n",
    "    temp = input\n",
    "    for i in range(6):\n",
    "        curr_col = i * 4\n",
    "        mean = torch.mean(temp[:, curr_col]).item()\n",
    "        min = torch.min(temp[:, curr_col]).item()\n",
    "        max = torch.max(temp[:, curr_col]).item()\n",
    "        mean_col = torch.full((50, 1), mean)\n",
    "        min_col = torch.full((50, 1), min)\n",
    "        max_col = torch.full((50, 1), max)\n",
    "\n",
    "        left = temp[:, :curr_col+1]\n",
    "        right = temp[:, curr_col+1:]\n",
    "        temp = torch.cat((left, mean_col, min_col, max_col, right), dim=1)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0763cce-3189-4e66-bf8a-6b1e9d18df80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(config, dataset):\n",
    "    es = EarlyStopping(patience=10)\n",
    "    net = Net(config[\"hidden_size\"])\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=config[\"lr\"], weight_decay=0.01)\n",
    "\n",
    "    trainloader, validloader = create_dataloaders(dataset, config[\"batch_size\"])\n",
    "\n",
    "    can_print = True\n",
    "    for epoch in range(config[\"max_num_epochs\"]):\n",
    "        val_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = torch.squeeze(inputs)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            if (len(outputs.size()) > 2): # squeeze if batch has more than 1 datapoint\n",
    "                outputs = torch.squeeze(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in validloader:\n",
    "                inputs, labels = data\n",
    "                inputs = torch.squeeze(inputs)\n",
    "                outputs = net(inputs)\n",
    "                if (len(outputs.size()) > 2):\n",
    "                    outputs = torch.squeeze(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            path = os.path.join(temp_checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save((net.state_dict()), path)\n",
    "            checkpoint = tune.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "            \n",
    "            tune.report(\n",
    "                {\"loss\": val_loss / len(validloader), \"accuracy\": correct / total}, checkpoint = checkpoint\n",
    "            )\n",
    "\n",
    "        if es.step(val_loss):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c43786d6-3784-4370-8a2c-fed34d66de77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(num_samples=160, max_num_epochs=100):\n",
    "    config = {\n",
    "        #\"batch_size\": tune.choice([2, 4, 8, 16, 32, 64]),\n",
    "        \"batch_size\": 32,\n",
    "        #\"hidden_size\": tune.choice([20 * i for i in range(1, 11)]),\n",
    "        \"hidden_size\": 100,\n",
    "        #\"lr\": tune.choice([0.1, 0.01, 0.001, 0.0001]),\n",
    "        \"lr\": 0.001,\n",
    "        \"max_num_epochs\": max_num_epochs\n",
    "    }\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        time_attr=\"training_iteration\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "    dataset = CustomDataset(\"labels.csv\", \"train_data\")\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train, dataset=dataset),\n",
    "            resources={\"cpu\": 1}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            #scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    \n",
    "    best_result = results.get_best_result()\n",
    "\n",
    "    print(f\"Best trial config: {best_result.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_result.metrics['loss']}\")\n",
    "    print(f\"Best trial final validation accuracy: {best_result.metrics['accuracy']}\")\n",
    "\n",
    "    return best_result, dataset.means, dataset.std_devs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e2686613-b7e4-4ea3-8f52-66d288ddc1ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-15 16:26:26</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:36.34        </td></tr>\n",
       "<tr><td>Memory:      </td><td>14.7/15.5 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 20.0/22 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
       "  \n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_8d90d_00000</td><td>RUNNING </td><td>127.0.0.1:32796</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         30.8556</td><td style=\"text-align: right;\">0.290239</td><td style=\"text-align: right;\">  0.887179</td></tr>\n",
       "<tr><td>train_8d90d_00001</td><td>RUNNING </td><td>127.0.0.1:524  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         30.5059</td><td style=\"text-align: right;\">0.273659</td><td style=\"text-align: right;\">  0.890598</td></tr>\n",
       "<tr><td>train_8d90d_00002</td><td>RUNNING </td><td>127.0.0.1:25352</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         30.8029</td><td style=\"text-align: right;\">0.282941</td><td style=\"text-align: right;\">  0.88547 </td></tr>\n",
       "<tr><td>train_8d90d_00003</td><td>RUNNING </td><td>127.0.0.1:34848</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         30.8765</td><td style=\"text-align: right;\">0.268634</td><td style=\"text-align: right;\">  0.895726</td></tr>\n",
       "<tr><td>train_8d90d_00004</td><td>RUNNING </td><td>127.0.0.1:20184</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         27.4485</td><td style=\"text-align: right;\">0.296554</td><td style=\"text-align: right;\">  0.88547 </td></tr>\n",
       "<tr><td>train_8d90d_00005</td><td>RUNNING </td><td>127.0.0.1:6948 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         28.9966</td><td style=\"text-align: right;\">0.298696</td><td style=\"text-align: right;\">  0.88547 </td></tr>\n",
       "<tr><td>train_8d90d_00006</td><td>RUNNING </td><td>127.0.0.1:35860</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         29.078 </td><td style=\"text-align: right;\">0.279256</td><td style=\"text-align: right;\">  0.88547 </td></tr>\n",
       "<tr><td>train_8d90d_00007</td><td>RUNNING </td><td>127.0.0.1:35672</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         28.0222</td><td style=\"text-align: right;\">0.269166</td><td style=\"text-align: right;\">  0.899145</td></tr>\n",
       "<tr><td>train_8d90d_00008</td><td>RUNNING </td><td>127.0.0.1:20568</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         28.3141</td><td style=\"text-align: right;\">0.335357</td><td style=\"text-align: right;\">  0.890598</td></tr>\n",
       "<tr><td>train_8d90d_00009</td><td>RUNNING </td><td>127.0.0.1:36300</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         28.4793</td><td style=\"text-align: right;\">0.280761</td><td style=\"text-align: right;\">  0.878632</td></tr>\n",
       "<tr><td>train_8d90d_00010</td><td>RUNNING </td><td>127.0.0.1:38076</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         29.4574</td><td style=\"text-align: right;\">0.293572</td><td style=\"text-align: right;\">  0.895726</td></tr>\n",
       "<tr><td>train_8d90d_00011</td><td>RUNNING </td><td>127.0.0.1:36072</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         22.7257</td><td style=\"text-align: right;\">0.344539</td><td style=\"text-align: right;\">  0.894017</td></tr>\n",
       "<tr><td>train_8d90d_00012</td><td>RUNNING </td><td>127.0.0.1:35056</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         28.0812</td><td style=\"text-align: right;\">0.458871</td><td style=\"text-align: right;\">  0.839316</td></tr>\n",
       "<tr><td>train_8d90d_00013</td><td>RUNNING </td><td>127.0.0.1:35132</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         28.7114</td><td style=\"text-align: right;\">0.228546</td><td style=\"text-align: right;\">  0.911111</td></tr>\n",
       "<tr><td>train_8d90d_00014</td><td>RUNNING </td><td>127.0.0.1:19208</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         21.9351</td><td style=\"text-align: right;\">0.424435</td><td style=\"text-align: right;\">  0.85641 </td></tr>\n",
       "<tr><td>train_8d90d_00015</td><td>RUNNING </td><td>127.0.0.1:35692</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         21.2465</td><td style=\"text-align: right;\">0.349599</td><td style=\"text-align: right;\">  0.888889</td></tr>\n",
       "<tr><td>train_8d90d_00016</td><td>RUNNING </td><td>127.0.0.1:21388</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         21.9784</td><td style=\"text-align: right;\">0.392563</td><td style=\"text-align: right;\">  0.873504</td></tr>\n",
       "<tr><td>train_8d90d_00017</td><td>RUNNING </td><td>127.0.0.1:35104</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         21.0046</td><td style=\"text-align: right;\">0.376098</td><td style=\"text-align: right;\">  0.895726</td></tr>\n",
       "<tr><td>train_8d90d_00018</td><td>RUNNING </td><td>127.0.0.1:35312</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         21.7688</td><td style=\"text-align: right;\">0.392081</td><td style=\"text-align: right;\">  0.876923</td></tr>\n",
       "<tr><td>train_8d90d_00019</td><td>RUNNING </td><td>127.0.0.1:35200</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         21.275 </td><td style=\"text-align: right;\">0.4484  </td><td style=\"text-align: right;\">  0.847863</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train pid=524)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=C:/Users/tjjtj/ray_results/train_2025-11-15_16-23-57/train_8d90d_00001_1_2025-11-15_16-24-50/checkpoint_000000)\n",
      "\u001b[36m(train pid=35312)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=C:/Users/tjjtj/ray_results/train_2025-11-15_16-23-57/train_8d90d_00018_18_2025-11-15_16-24-50/checkpoint_000000)\u001b[32m [repeated 15x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "2025-11-15 16:26:26,118\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "\u001b[36m(train pid=35132)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=C:/Users/tjjtj/ray_results/train_2025-11-15_16-23-57/train_8d90d_00013_13_2025-11-15_16-24-50/checkpoint_000001)\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "2025-11-15 16:26:26,439\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/tjjtj/ray_results/train_2025-11-15_16-23-57' in 0.2965s.\n",
      "2025-11-15 16:26:30,441\tINFO tune.py:1041 -- Total run time: 100.52 seconds (96.03 seconds for the tuning loop).\n",
      "2025-11-15 16:26:30,441\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"C:/Users/tjjtj/ray_results/train_2025-11-15_16-23-57\", trainable=...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'batch_size': 32, 'hidden_size': 100, 'lr': 0.001, 'max_num_epochs': 100}\n",
      "Best trial final validation loss: 0.22854599748787127\n",
      "Best trial final validation accuracy: 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "def test_model(net, log_file, means, std_devs):    \n",
    "    gestures = [\"stationary\", \"random\", \"wave\", \"punch\", \"swipe\", \"wipe\", \"shake\", \"hammer\"]\n",
    "    for gesture in gestures:\n",
    "        datafile = gesture + \"_labels.csv\"\n",
    "        dataset = CustomDataset(datafile, \"test_data\", means, std_devs)\n",
    "        testloader = DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                inputs, labels = data\n",
    "                inputs = torch.squeeze(inputs)\n",
    "                outputs = net(inputs)\n",
    "                if (len(outputs.size()) > 2): # squeeze if batch has more than 1 datapoint\n",
    "                    outputs = torch.squeeze(outputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        log = f'Accuracy of the network on {gesture}: {100 * correct // total}%\\n'\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(log)\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\tjjtj\\\\CG4002\\\\glove_data\")\n",
    "log_file_basename = \"log\"\n",
    "extension = \".txt\"\n",
    "counter = 1\n",
    "while True:\n",
    "    log_file = f\"{log_file_basename}_{counter}{extension}\"\n",
    "    if not os.path.exists(log_file):\n",
    "        break\n",
    "    counter += 1\n",
    "\n",
    "for i in range(20):\n",
    "    best_result, means, std_devs = main(num_samples=20, max_num_epochs=100)    \n",
    "    best_trained_model = Net(best_result.config[\"hidden_size\"])\n",
    "    \n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "    model_state = torch.load(checkpoint_path, weights_only=True)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    save_basename = \"saved_model\"\n",
    "    save_ext = \".pt\"\n",
    "    counter = 1\n",
    "    while True:\n",
    "        save_file = f\"{save_basename}_{counter}{save_ext}\"\n",
    "        if not os.path.exists(save_file):\n",
    "            break\n",
    "        counter += 1\n",
    "    torch.save(best_trained_model.state_dict(), save_file)\n",
    "\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"Model {i}\\n\")\n",
    "        f.write(f\"Means {means}\\n\")\n",
    "        f.write(f\"Std devs {std_devs}\\n\")\n",
    "    test_model(best_trained_model, log_file, means, std_devs)\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
